Although seemingly gender-neutral,  voice assistants such as Alexa, Siri and many other voice assistants, are anthropomorphised as women with female-sounding voices (Anshu Roja Selvamani, 2025).  With over 90% of the default voice of these assistants being set to female, it is crucial to evaluate how they depict women (Chin-Rothmann and Robison, 2020). 

In 2017, journalist Leah Fessler investigated how a multitude of voice assistants responded to sexual harassment and beratment (Anshu Roja Selvamani, 2025). In the experiment, she discovered that the voice assistant’s responses were subservient, evasive and surprisingly grateful (Anshu Roja Selvamani, 2025). For example, when called the “B” word, Siri responded with “I’d blush if I could”, while Alexa responded with, “Thanks for the feedback” (Anshu Roja Selvamani, 2025). Although Leah continued to berate these voice assistants with a series of inappropriate comments, she was met with very similar, civil responses (Anshu Roja Selvamani, 2025). When personified with feminine voices and characteristics, this behavior reinforces subservience in digital form (Anshu Roja Selvamani, 2025). 

Many academic studies have suggested that gendered voice assistants can alter a user's attitude or perceptions of a person or situation (Chin-Rothmann and Robison, 2020). For instance, Nass et. al found that gendered computer voices alone are enough to elicit gender-sterotypic behaviors from users, even without any gender cues -  like appearance (Chin-Rothmann and Robison, 2020). With less than 30% of the AI workforce being composed of women, these systems will continue to perpetuate bias (Anshu Roja Selvamani, 2025). As AI ethicist Josie Young said,“when we add a human name, face, or voice [to technology] … it reflects the biases in the viewpoints of the teams that built it” (Young, 2018). When bias seeps into algorithms, it shapes lives, opportunities, and can lead to negative consequences for vulnerable populations (Anshu Roja Selvamani, 2025).  

Vulnerable populations being possibly harmed by voice assistants is not unheard of, with 45% of smart speaker users being concerned about privacy, and 42% concerned about voice data hacking (Cherkassky, 2025). While voice-based technology evolves and develops, the threat of voice data privacy remains (Cherkassky, 2025). Most voice recognition systems send and store recordings by people in the cloud to process and transmit responses back to the user (Cherkassky, 2025). It is very often that these systems use collected data to train algorithms to improve the accuracy of automatic speech recognition (Cherkassky, 2025). Some data collected may be more important than others, like a doctor’s note-taking voice assistant may have collected classified data (Cherkassky, 2025). 

The recent developments in voice technology have made things more complex and incorporated more code, which has opened the door for hackers to break into devices and systems (Cherkassky, 2025). Using cloud services in voice applications can lead to downsides related to security, safety and privacy (Cherkassky, 2025). If a cybercriminal gained access to stored data or cloud systems, they could access recorded conversations and sensitive information (Cherkassky, 2025). Furthermore, criminals could use this voice data as a biometric identification factor against individuals and organisations(Cherkassky, 2025). 
Voice-related information generated through voice recognition is biometric data that could potentially identify a person (Cherkassky, 2025). This biometric information is personal information under various privacy and security laws. When biometric data is stored locally, it may not be a risk of privacy (Cherkassky, 2025). Regardless, abuse can still happen when tech companies that use voice recognition store data to store in a cloud (Cherkassky, 2025) . 

Many businesses use cloud computing for security, however cybercriminals can attack cloud computing systems in many different ways, which include: data breaches, insider threats and hijacked accounts (Cherkassky, 2025). Cybercriminals can view, copy and transmit voice data in data breaches. This can cause a loss of trust in customers and heavy penalties for businesses (Cherkassky, 2025). However, 60% of data breaches are insider attacks from experienced IT users, contractors, consultants and employees (Cherkassky, 2025). Fraud and monetary gain are usually the motivation of insider attacks, while other threats to cloud-stored data come from human error or negligence (Cherkassky, 2025). 

The concern for privacy is one of the most critical challenges that tech companies face when managing voice assistants (Cherkassky, 2025). Voice related data stored in precise locations poses many risks, as it can be misused to display sensitive information or even commit identity fraud (Cherkassky, 2025). 

References:


Anshu Roja Selvamani (2025). Advancing Women & Technology: Empowering Futures Through Digital Skills: 2025: News and Resources: About Us: Center of Excellence for Women & Technology: Indiana University Bloomington. [online] Center of Excellence for Women & Technology. Available at: https://womenandtech.indiana.edu/about/news/2025/mind-the-gap.html [Accessed 19 Nov. 2025]

Cherkassky, D. (2025). The Voice Privacy Problem - Kardome. [online] Kardome. Available at: https://www.kardome.com/resources/blog/voice-privacy-concerns/ [Accessed 19 Nov. 2025].

Chin-Rothmann, C. and Robison, M. (2020). How AI bots and voice assistants reinforce gender bias. [online] Brookings. Available at: https://www.brookings.edu/articles/how-ai-bots-and-voice-assistants-reinforce-gender-bias/. [Accessed 19 Nov. 2025].